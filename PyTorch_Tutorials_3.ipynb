{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "ニューラルネットワークは`torch.nn`パッケージで構築することができます。\n",
    "`autograd`は前回のチュートリアルで見たと思いますが，`nn`はモデルを定義したり，そのモデルの微分計算をするので，`autograd`に依存するパッケージです。\n",
    "`nn.Module`には，層とメソッド`forward(input)`が含まれており，`forward(input)`は`output`をかえします。\n",
    "\n",
    "例えば，手書き文字を分類するニューラルネットワークを見てみましょう：\n",
    "\n",
    "![http://pytorch.org/tutorials/_images/mnist.png](http://pytorch.org/tutorials/_images/mnist.png)\n",
    "convnet\n",
    "\n",
    "これはシンプルな順伝播ネットワークです。つまり，リカレントニューラルネットワークのように，後の層の出力が前の層に戻ることがない，一方向のネットワークです。\n",
    "\n",
    "ニューラルネットワークの学習は，大体次のような手続きで行います：\n",
    "- 学習可能なパラメータをもつニューラルネットワークを定義する。\n",
    "- データを入力し，入力のたびに学習を繰り返す。\n",
    "- 損失計算を行う。損失は，教師データと出力がどれだけ離れているかを示す。\n",
    "- 誤差逆伝播法による勾配計算\n",
    "- ネットワークのパラメータを更新する。更新式は，次式のようになる：\n",
    "$w = w - \\alpha \\nabla L$\n",
    "\n",
    "### ネットワークを定義する\n",
    "それでは，実際にネットワークを定義してみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d (6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120)\n",
      "  (fc2): Linear(in_features=120, out_features=84)\n",
      "  (fc3): Linear(in_features=84, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward`関数を定義するだけで，`backward`関数は自動的に定義できます。この`backward`関数というのは，勾配が計算される関数です。これが`autograd`の便利なところです。\n",
    "`forward`関数では，Tensorに関する全ての演算を使うことができます。\n",
    "***\n",
    "学習可能なパラメータは`net.parameters()`で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward関数の入力も出力も両方`autograd.Variable`です。\n",
    "上記のネットワーク(LeNet)の入力は32×32なので，もしMNISTデータセットでこのネットワークを使うなら32×32にリサイズする必要があります。\n",
    "***\n",
    "全てのパラメータの勾配情報を溜め込んでいるバッファーを0にし，ランダムな勾配を逆伝播させるには："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0732  0.1061  0.1426 -0.1083 -0.0080  0.0531 -0.0105  0.0335 -0.0270  0.0096\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "`torch.nn`はミニバッチのみサポートしているので，単体のデータを利用できません。\n",
    "例えば，`nn.Conv2d`は４次元テンソル`nSamples x nChannels x Height x Width`を引数としてとります。単体のサンプルを入力するときは，`input.unsqueeze(0)`を使うことでバッチの次元をごまかすことができます。\n",
    "\n",
    "***\n",
    "\n",
    "一度，これまでのおさらいをします。\n",
    "\n",
    "- `torch.Tensor` - 多次元配列\n",
    "- `autograd.Variable` - Tensorのラッパーで，Tensorに行われた演算を記録する。`Tensor`と同様のAPIを持ち，`backward()`のような追加のメソッドなどもある。tensorに関する勾配情報も保持している。\n",
    "- `nn.Module` - ニューラルネットワークのモジュール。パラメータをカプセル化したり，パラメータをGPUに載せたり，出力したり，ロードしたりする便利な機能をもつ。\n",
    "- `nn.Parameter` - Variableの一種で，`Module`を定義した時に自動的に登録される。\n",
    "- `autograd.Function` - autogradの演算における順伝播と逆伝播の定義を実装したパッケージ。すべての`Variable`の演算は，少なくとも１つの`Function`ノードを作る。そしてこのノードは，`Variable`を生成した関数と結び付けられる。そしてこのノードが，演算の履歴を再現する。\n",
    "\n",
    "ここまで，\n",
    "- ニューラルネットワークを定義する\n",
    "- 入力の処理とbackwardの呼び出し\n",
    "について見てきました。\n",
    "\n",
    "次に，\n",
    "- 損失の計算\n",
    "- ネットワークの更新\n",
    "について見ていきます。\n",
    "\n",
    "## 損失関数\n",
    "損失関数は (output, target) という２つの入力を受け取り，outputがtargetとどれだけ離れているかを計算します。\n",
    "\n",
    "`nn`パッケージには，いくつかの損失関数が実装されています。シンプルな損失関数として，`nn.MSELoss`を取り上げてみます。これは，入力とターゲットの平均二乗誤差を計算するものです。\n",
    "\n",
    "例えば，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.3841\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は，`loss`がどのように計算されているかについてのフローです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    input  -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "\n",
    "           -> view -> linear -> relu -> linear -> relu -> linear\n",
    "\n",
    "           -> MSELoss\n",
    "\n",
    "           -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.backward`を呼び出した時，計算フロー全体は損失に関して微分される。そして，フロー（グラフ中）のVariableは`.grad`Variableを持つ。\n",
    "\n",
    "例として，損失関数から前の計算ステップにさかのぼってみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000002A1F10A1400>\n",
      "<AddmmBackward object at 0x000002A1F10A1390>\n",
      "<ExpandBackward object at 0x000002A1F10A1400>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逆伝播\n",
    "誤差を逆伝播させるためには，`loss.backward()`するだけで良いです。\n",
    "`net.zero_grad()`で勾配をすべて0にすることができますが，これを呼び出さなければ勾配は既存の勾配に蓄積されていきます。\n",
    "\n",
    "それでは`loss.backward()`を実際によびだして，その前後で`conv1`のバイアスの勾配がどのように変わるか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.6753\n",
      "  9.1669\n",
      "  2.7569\n",
      " -4.2307\n",
      "  2.5546\n",
      " -2.6041\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で，損失関数の扱い方を見てきました。\n",
    "\n",
    "### 後で読みたい：\n",
    "ニューラルネットワークのパッケージには，種々のモジュールと損失関数が用意されています。そしてそれらを組み合わせて深層ニューラルネットワークを構築します。それらのモジュールについては[こちら](http://pytorch.org/docs/nn)。\n",
    "\n",
    "### 残っていること\n",
    "- ネットワークのパラメータの更新\n",
    "\n",
    "## パラメータの更新\n",
    "最も簡単なパラメータ更新方法としては，確率的勾配降下法(SGD)が知られています。これは次の式で表現できます：\n",
    "`weight = weight - learning_rate * gradient`\n",
    "\n",
    "この更新規則は，次のコードで表現できます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし，SGD以外にもNesterov-SGD, Adam, RMSPropなどのパラメータ更新法を適用したい人もいるでしょう。これを可能にするのが`torch.optim`パッケージです。このパッケージにはすでにこれらの更新規則が実装されており，簡単に使うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
