{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "ニューラルネットワークは`torch.nn`パッケージで構築することができます。\n",
    "`autograd`は前回のチュートリアルで見たと思いますが，`nn`はモデルを定義したり，そのモデルの微分計算をするので，`autograd`に依存するパッケージです。\n",
    "`nn.Module`には，層とメソッド`forward(input)`が含まれており，`forward(input)`は`output`をかえします。\n",
    "\n",
    "例えば，手書き文字を分類するニューラルネットワークを見てみましょう：\n",
    "\n",
    "![http://pytorch.org/tutorials/_images/mnist.png](http://pytorch.org/tutorials/_images/mnist.png)\n",
    "convnet\n",
    "\n",
    "これはシンプルな順伝播ネットワークです。つまり，リカレントニューラルネットワークのように，後の層の出力が前の層に戻ることがない，一方向のネットワークです。\n",
    "\n",
    "ニューラルネットワークの学習は，大体次のような手続きで行います：\n",
    "- 学習可能なパラメータをもつニューラルネットワークを定義する。\n",
    "- データを入力し，入力のたびに学習を繰り返す。\n",
    "- 損失計算を行う。損失は，教師データと出力がどれだけ離れているかを示す。\n",
    "- 誤差逆伝播法による勾配計算\n",
    "- ネットワークのパラメータを更新する。更新式は，次式のようになる：\n",
    "$w = w - \\alpha \\nabla L$\n",
    "\n",
    "### ネットワークを定義する\n",
    "それでは，実際にネットワークを定義してみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d (6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120)\n",
      "  (fc2): Linear(in_features=120, out_features=84)\n",
      "  (fc3): Linear(in_features=84, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward`関数を定義するだけで，`backward`関数は自動的に定義できます。この`backward`関数というのは，勾配が計算される関数です。これが`autograd`の便利なところです。\n",
    "`forward`関数では，Tensorに関する全ての演算を使うことができます。\n",
    "***\n",
    "学習可能なパラメータは`net.parameters()`で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward関数の入力も出力も両方`autograd.Variable`です。\n",
    "上記のネットワーク(LeNet)の入力は32×32なので，もしMNISTデータセットでこのネットワークを使うなら32×32にリサイズする必要があります。\n",
    "***\n",
    "全てのパラメータの勾配情報を溜め込んでいるバッファーを0にし，ランダムな勾配を逆伝播させるには："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0469  0.0944 -0.0326  0.1442 -0.0253  0.0428  0.0319  0.0414 -0.0681  0.0572\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "`torch.nn`はミニバッチのみサポートしているので，単体のデータを利用できません。\n",
    "例えば，`nn.Conv2d`は４次元テンソル`nSamples x nChannels x Height x Width`を引数としてとります。単体のサンプルを入力するときは，`input.unsqueeze(0)`を使うことでバッチの次元をごまかすことができます。\n",
    "\n",
    "***\n",
    "\n",
    "一度，これまでのおさらいをします。\n",
    "\n",
    "- `torch.Tensor` - 多次元配列\n",
    "- `autograd.Variable` - Tensorのラッパーで，Tensorに行われた演算を記録する。`Tensor`と同様のAPIを持ち，`backward()`のような追加のメソッドなどもある。tensorに関する勾配情報も保持している。\n",
    "- `nn.Module` - ニューラルネットワークのモジュール。パラメータをカプセル化したり，パラメータをGPUに載せたり，出力したり，ロードしたりする便利な機能をもつ。\n",
    "- `nn.Parameter` - Variableの一種で，`Module`を定義した時に自動的に登録される。\n",
    "- `autograd.Function` - autogradの演算における順伝播と逆伝播の定義を実装したパッケージ。すべての`Variable`の演算は，少なくとも１つの`Function`ノードを作る。そしてこのノードは，`Variable`を生成した関数と結び付けられる。そしてこのノードが，演算の履歴を再現する。\n",
    "\n",
    "ここまで，\n",
    "- ニューラルネットワークを定義する\n",
    "- 入力の処理とbackwardの呼び出し\n",
    "について見てきました。\n",
    "\n",
    "次に，\n",
    "- 損失の計算\n",
    "- ネットワークの更新\n",
    "について見ていきます。\n",
    "\n",
    "## 損失関数\n",
    "損失関数は (output, target) という２つの入力を受け取り，outputがtargetとどれだけ離れているかを計算します。\n",
    "\n",
    "`nn`パッケージには，いくつかの損失関数が実装されています。シンプルな損失関数として，`nn.MSELoss`を取り上げてみます。これは，入力とターゲットの平均二乗誤差を計算するものです。\n",
    "\n",
    "例えば，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.2517\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は，`loss`がどのように計算されているかについてのフローです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    input  -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "\n",
    "           -> view -> linear -> relu -> linear -> relu -> linear\n",
    "\n",
    "           -> MSELoss\n",
    "\n",
    "           -> loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
